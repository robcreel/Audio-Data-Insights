{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Spotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ryan Bales (@ryanbales)<br>ryan@balesofdata.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/2016_debates/\"\n",
    "transcription_data_path = \"{}{}\".format(data_path, \"transcripts/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(file_list):\n",
    "    data = []\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            transcript = json.load(f)\n",
    "            data.append(transcript[\"results\"][\"transcripts\"][0][\"transcript\"])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Transcription Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    \"{}{}\".format(transcription_data_path, \"debate_1.mp3.json\"),\n",
    "    \"{}{}\".format(transcription_data_path, \"debate_2.mp3.json\"),\n",
    "    \"{}{}\".format(transcription_data_path, \"debate_3.mp3.json\"),\n",
    "    \"{}{}\".format(transcription_data_path, \"vp_debate.mp3.json\")\n",
    "]\n",
    "                       \n",
    "docs = get_text(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Documents (Remove Stop Words and Run Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "for doc in docs:\n",
    "    doc = gensim.parsing.preprocessing.remove_stopwords(doc)\n",
    "    doc = gensim.parsing.preprocessing.stem_text(doc)\n",
    "    doc = gensim.parsing.preprocessing.strip_punctuation(doc)\n",
    "\n",
    "    processed_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_phrase = \"trump\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Same Preporcessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kjoio'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_phrase = gensim.parsing.preprocessing.remove_stopwords(search_phrase)\n",
    "search_phrase = gensim.parsing.preprocessing.stem_text(search_phrase)\n",
    "search_phrase = gensim.parsing.preprocessing.strip_punctuation(search_phrase)\n",
    "\n",
    "search_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_phrase.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for Keywords in each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def search_keyword(doc, search_phrase):\n",
    "    tokens = gensim.utils.tokenize(doc)\n",
    "    return Counter(tokens)[search_phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from gensim.test.utils import datapath\n",
    ">>> from gensim.models.word2vec import Text8Corpus\n",
    ">>> from gensim.models.phrases import Phrases, Phraser\n",
    ">>>\n",
    ">>> sentences = Text8Corpus(datapath('testcorpus.txt'))\n",
    ">>> phrases = Phrases(sentences, min_count=1, threshold=1)\n",
    ">>>\n",
    ">>> bigram = Phraser(phrases)\n",
    ">>> sent = [u'trees', u'graph', u'minors']\n",
    ">>> print(bigram[sent])\n",
    "[u'trees_graph', u'minors'\n",
    " \n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def search_bigrams(doc, search_phrase):\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_trigrams(doc, search_phrase):\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 usages of 'kjoio' in document index 1\n",
      "Found 0 usages of 'kjoio' in document index 2\n",
      "Found 0 usages of 'kjoio' in document index 3\n",
      "Found 0 usages of 'kjoio' in document index 4\n"
     ]
    }
   ],
   "source": [
    "search_phrase_length = len(search_phrase.split())\n",
    "doc_index = 0\n",
    "\n",
    "for doc in processed_docs:\n",
    "    doc_index += 1\n",
    "    \n",
    "    if search_phrase_length == 1:\n",
    "        phrase_count = search_keyword(doc, search_phrase)\n",
    "    elif search_phrase_length == 2:\n",
    "        phrase_count = search_bigrams(doc, search_phrase)\n",
    "    elif search_phrase_length == 3:\n",
    "        phrase_count = search_bigrams(doc, search_phrase)\n",
    "    else:\n",
    "        raise Exception(\"Unsupported Search Phrase\")\n",
    "    \n",
    "    print(\"Found {} usages of '{}' in document index {}\".format(phrase_count, search_phrase, doc_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-data-insights",
   "language": "python",
   "name": "audio-data-insights"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
